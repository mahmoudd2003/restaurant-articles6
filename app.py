# Ø¶Ù…Ø§Ù† Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ®Ø²ÙŠÙ†
import os
os.makedirs("data", exist_ok=True)

# Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ (Ø­Ø³Ø¨ Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ø¹Ù†Ø¯Ùƒ)
try:
    from category_criteria import get_category_criteria
except ImportError:
    from modules.category_criteria import get_category_criteria  # Ù„Ùˆ Ù†Ù‚Ù„ØªÙ‡ Ø¯Ø§Ø®Ù„ utils/modules

import io, csv, unicodedata, json
from datetime import datetime
from pathlib import Path

import streamlit as st

from utils.openai_client import get_client, chat_complete
from utils.exporters import to_docx, to_json
from utils.content_fetch import fetch_and_extract
from utils.competitor_analysis import analyze_competitors, extract_gap_points
from utils.quality_checks import quality_report
from utils.llm_reviewer import llm_review, llm_fix

# Ø¥Ø¶Ø§ÙØ§Øª ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³
import requests
import markdown as md  # Ù„ØªØ­ÙˆÙŠÙ„ Markdown Ø¥Ù„Ù‰ HTML Ù‚Ø¨Ù„ Ø§Ù„Ù†Ø´Ø±

# ========== Helpers for Places Integration (normalize + protected details) ==========
import re, unicodedata as _ud
from difflib import SequenceMatcher
from urllib.parse import urlparse  # Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ

_AR_DIAC = re.compile(r'[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]')
_PUNCT  = re.compile(r'[^\w\s\u0600-\u06FF]')

def normalize_ar(s: str) -> str:
    if not s: return ""
    s = _ud.normalize("NFKC", s)
    s = _AR_DIAC.sub("", s)
    s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§").replace("Ù‰","ÙŠ")
    s = s.replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ").replace("Ø©","Ù‡").replace("Ù€","")
    s = _PUNCT.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()
    trans = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")
    s = s.translate(trans)
    return s

def best_match(name: str, index: dict, threshold: float = 0.90):
    key = normalize_ar(name)
    if key in index:
        return index[key]
    best_key, best_score = None, 0.0
    for k in index.keys():
        sc = SequenceMatcher(None, key, k).ratio()
        if sc > best_score:
            best_key, best_score = k, sc
    return index.get(best_key) if best_score >= threshold else None

def _fmt(v):
    return str(v).strip() if v and str(v).strip() else "ØºÙŠØ± Ù…ØªÙˆÙØ±"

def _link(label, url):
    return f"[{label}]({url})" if url and str(url).strip() else "ØºÙŠØ± Ù…ØªÙˆÙØ±"

def render_details_block(item: dict) -> str:
    address = _fmt(item.get("address"))
    phone   = _fmt(item.get("phone"))
    hours   = _fmt(item.get("thursday_hours"))
    family  = _fmt(item.get("family_friendly"))  # "Ù†Ø¹Ù… (ØªÙ‚Ø¯ÙŠØ±ÙŠ)" / "Ù„Ø§ (ØªÙ‚Ø¯ÙŠØ±ÙŠ)" / ØºÙŠØ± Ù…ØªÙˆÙØ±
    pricepp = _fmt(item.get("price_per_person"))
    dish    = _fmt(item.get("signature_dish"))   # "â€”" Ø£Ùˆ Ø§Ø³Ù… Ø·Ø¨Ù‚
    busy    = _fmt(item.get("busy_times"))
    mapslnk = _link("ÙØªØ­ ÙÙŠ Ø®Ø±Ø§Ø¦Ø· Google", item.get("maps_url"))
    webslnk = _link("Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹", item.get("website"))
    return (
        "\n**ØªÙØ§ØµÙŠÙ„ Ø¹Ù…Ù„ÙŠØ©:**\n"
        f"- **Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:** {address}\n"
        f"- **Ø§Ù„Ù‡Ø§ØªÙ:** {phone}\n"
        f"- **Ø§Ù„Ø£ÙˆÙ‚Ø§Øª:** {hours}\n"
        f"- **Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹ÙˆØ§Ø¦Ù„:** {family}\n"
        f"- **Ø§Ù„Ø³Ø¹Ø± Ù„Ù„Ø´Ø®Øµ:** {pricepp}\n"
        f"- **Ø§Ù„Ø·Ø¨Ù‚ Ø§Ù„Ù…Ù…ÙŠØ²:** {dish}\n"
        f"- **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø²Ø­Ù…Ø©:** {busy}\n"
        f"- **Ø®Ø±Ø§Ø¦Ø· Google:** {mapslnk}\n"
        f"- **Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ:** {webslnk}\n"
    )

def inject_details_under_h3(markdown_text: str, places_index: dict) -> str:
    """
    Ø¨Ø¹Ø¯ ÙƒÙ„ '### <Ø§Ø³Ù… Ø§Ù„Ù…Ø·Ø¹Ù…>' ÙˆØ§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ ØªÙ„ÙŠÙ‡ØŒ Ø£Ø¯Ø±Ø¬ ÙƒØªÙ„Ø© 'ØªÙØ§ØµÙŠÙ„ Ø¹Ù…Ù„ÙŠØ©'
    Ø¨Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø§Ø³Ù… Ù…Ø¹ places_index (Ù…Ø­Ù…ÙŠØ© 100%). Ø¥Ù† Ù„Ù… Ù†Ø¬Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©ØŒ Ù†Ø¹Ø±Ø¶ 'ØºÙŠØ± Ù…ØªÙˆÙØ±'.
    """
    if not markdown_text or not places_index:
        return markdown_text

    lines = markdown_text.splitlines()
    out = []
    i = 0
    while i < len(lines):
        line = lines[i]
        out.append(line)

        if line.startswith("### "):
            h3_name = line[4:].strip()
            # Ø§Ø­ØªÙØ¸ Ø¨Ø£ÙŠ Ø£Ø³Ø·Ø± ÙØ§Ø±ØºØ© Ø¨Ø¹Ø¯ H3
            j = i + 1
            while j < len(lines) and lines[j].strip() == "":
                out.append(lines[j]); j += 1
            # Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ (Ø­ØªÙ‰ Ø³Ø·Ø± ÙØ§Ø±Øº Ø£Ùˆ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯ÙŠØ¯)
            while j < len(lines) and not lines[j].startswith("#") and lines[j].strip() != "":
                out.append(lines[j]); j += 1

            matched = best_match(h3_name, places_index, threshold=0.90)
            if matched is None:
                matched = {
                    "address": None, "phone": None, "thursday_hours": None,
                    "family_friendly": None, "price_per_person": None,
                    "signature_dish": "â€”", "busy_times": None,
                    "maps_url": None, "website": None
                }
            out.append(render_details_block(matched))
            i = j
            continue

        i += 1

    return "\n".join(out)
# ========== End Helpers ======================================================

# ========== (Ø¬Ø¯ÙŠØ¯) Helpers Ù„ÙØ±Ø¶ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ù„Ù„Ù…Ø³Ø§Øª ÙˆØ§Ù„Ø·ÙˆÙ„ ==========
def _word_count(md: str) -> int:
    """Ø¹Ø¯Ù‘ ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø¨ÙŠ/Ù„Ø§ØªÙŠÙ†ÙŠ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù…Ø§Ø±ÙƒØ¯Ø§ÙˆÙ† Ø¨Ø³ÙŠØ·Ø©."""
    txt = re.sub(r"[#>*_`~\[\]\(\)]", " ", md or "")
    tokens = re.findall(r"[\u0600-\u06FF\w]+", txt, flags=re.UNICODE)
    return len(tokens)

FAQ_PATTERNS = [
    r"^##\s*FAQ\s*$",
    r"^##\s*Ø§Ù„Ø£Ø³Ø¦Ù„Ø©\s+Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\s*$",
    r"^##\s*Ø£Ø³Ø¦Ù„Ø©\s+Ø´Ø§Ø¦Ø¹Ø©\s*$",
]
METH_PATTERNS = [
    r"^##\s*Ù…Ù†Ù‡Ø¬ÙŠØ©\s*Ø§Ù„ØªØ­Ø±ÙŠØ±\s*$",
    r"^##\s*Ù…Ù†Ù‡Ø¬ÙŠØ©\s*$",
    r"^##\s*Ø·Ø±ÙŠÙ‚Ø©\s*Ø§Ù„Ø¹Ù…Ù„\s*$",
]

def _has_section(md: str, patterns) -> bool:
    if not md: return False
    for p in patterns:
        if re.search(p, md, flags=re.IGNORECASE | re.MULTILINE | re.UNICODE):
            return True
    return False

PROMPTS_DIR = Path("prompts")
def read_prompt(name: str) -> str:
    return (PROMPTS_DIR / name).read_text(encoding="utf-8")

def _read_template_file(name: str) -> str:
    """ÙŠÙ‚Ø±Ø£ Ù…Ù† prompts/<name>ØŒ ÙˆØ¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø¬Ø±Ù‘Ø¨ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¬Ø°Ø±ÙŠ."""
    try:
        return read_prompt(name)
    except Exception:
        p = Path(name)
        return p.read_text(encoding="utf-8") if p.exists() else ""

def ensure_sections(article_md: str, need_faq: bool, need_meth: bool) -> str:
    """ÙŠØ¯Ø±Ø¬ FAQ/Ù…Ù†Ù‡Ø¬ÙŠØ© Ø¥Ù† ØºØ§Ø¨Ø§ØŒ Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ faq.md/methodology.md."""
    out = article_md or ""
    if need_faq and not _has_section(out, FAQ_PATTERNS):
        faq_block = _read_template_file("faq.md").strip()
        if faq_block:
            out += ("\n\n" + faq_block + "\n")
    if need_meth and not _has_section(out, METH_PATTERNS):
        meth_block = _read_template_file("methodology.md").strip()
        if meth_block:
            out += ("\n\n" + meth_block + "\n")
    return out

def add_human_fallback(article_md: str) -> str:
    """Fallback Ø¨Ø³ÙŠØ· Ù„Ùˆ Ù„Ù… ØªÙ†Ø¬Ø­ Ø·Ø¨Ù‚Ø© Polish."""
    if re.search(r"^##\s*Ù„Ù…Ø³Ø§Øª\s+Ø¨Ø´Ø±ÙŠØ©\s*$", article_md or "", flags=re.MULTILINE):
        return article_md
    extra = (
        "\n\n## Ù„Ù…Ø³Ø§Øª Ø¨Ø´Ø±ÙŠØ©\n"
        "â€¢ ØªØ¬Ø§Ø±Ø¨ Ù…Ø¨Ø§Ø´Ø±Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚: Ø²ÙŠØ§Ø±Ø§Øª ÙØ¹Ù„ÙŠØ© ÙˆØªØ°ÙˆÙ‘Ù‚ Ø£Ø·Ø¨Ø§Ù‚ Ù…Ø­Ø¯Ø¯Ø©.\n"
        "â€¢ Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø©: Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ù…ÙˆØ«Ù‚Ø© ÙˆØ¨ÙŠØ§Ù†Ø§Øª Ø±Ø³Ù…ÙŠØ© Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹.\n"
        "â€¢ ØªÙØ§ØµÙŠÙ„ Ø¹Ù…Ù„ÙŠØ©: Ø³Ø¹Ø± ØªÙ‚Ø±ÙŠØ¨ÙŠØŒ Ø£ÙØ¶Ù„ ÙˆÙ‚Øª Ù„Ù„Ø²ÙŠØ§Ø±Ø©ØŒ Ù†ØµÙŠØ­Ø© Ù„Ù„Ø°Ø±ÙˆØ©.\n"
        "â€¢ Ø´ÙØ§ÙÙŠØ©: Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡ Ù„Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØºÙŠÙ‘Ø± Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…/Ø§Ù„Ø£Ø³Ø¹Ø§Ø±.\n"
    )
    return (article_md or "") + extra

def enforce_word_target_via_llm(client, primary_model, fallback_model, article_md: str,
                                target_words: int, tolerance_pct: int = 12) -> str:
    """ÙŠÙˆØ³Ù‘Ø¹/ÙŠØ®ØªØµØ± Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆFAQ/Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©."""
    wc = _word_count(article_md)
    low = int(target_words * (1 - tolerance_pct/100))
    high = int(target_words * (1 + tolerance_pct/100))
    if low <= wc <= high:
        return article_md

    if wc < low:
        delta = low - wc
        instr = (
            f"ÙˆØ³Ù‘Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø­Ùˆ ~{delta} ÙƒÙ„Ù…Ø© Ù„ÙŠØµØ¨Ø­ Ù‚Ø±ÙŠØ¨Ù‹Ø§ Ù…Ù† {target_words} ÙƒÙ„Ù…Ø©ØŒ "
            "Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ© ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙƒÙ…Ø§ Ù‡ÙŠ. Ø£Ø¶Ù ØªÙØ§ØµÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ù…ÙˆØ«ÙˆÙ‚Ø© "
            "ÙˆØªØ¬Ù†Ù‘Ø¨ Ø§Ù„Ø­Ø´Ùˆ ÙˆØ§Ù„Ù…Ø¬Ø§Ø². Ù„Ø§ ØªØ­Ø°Ù Ù‚Ø³Ù…ÙŠ FAQ ÙˆÙ…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ø±ÙŠØ± Ø¥Ù† ÙˆÙØ¬Ø¯Ø§."
        )
    else:
        delta = wc - high
        instr = (
            f"Ø§Ø®ØªØµØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø­Ùˆ ~{delta} ÙƒÙ„Ù…Ø© Ù„ÙŠØµØ¨Ø­ Ù‚Ø±ÙŠØ¨Ù‹Ø§ Ù…Ù† {target_words} ÙƒÙ„Ù…Ø©ØŒ "
            "Ù…Ø¹ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ø­Ø´Ùˆ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ© ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†. Ù„Ø§ ØªÙ„Ù…Ø³ FAQ ÙˆÙ…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ø±ÙŠØ±."
        )

    messages = [
        {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ ÙŠÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¥ÙŠØ¬Ø§Ø² ÙˆØ§Ù„Ø¥Ø´Ø¨Ø§Ø¹ ÙˆÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ©."},
        {"role": "user", "content": f"Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø§Ù„ÙŠ (Ù„Ø§ ØªØºÙŠÙ‘Ø± Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†):\n\n{article_md}\n\nØ§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n{instr}"},
    ]
    try:
        refined = chat_complete(client, messages, max_tokens=2400, temperature=0.3,
                                model=primary_model, fallback_model=fallback_model)
        return refined or article_md
    except Exception:
        return article_md
# ========== End enforce helpers =============================================

st.set_page_config(page_title="Ù…ÙˆÙ„Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (E-E-A-T)", page_icon="ğŸ½ï¸", layout="wide")
st.title("ğŸ½ï¸ Ù…ÙˆÙ„Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù… â€” E-E-A-T + Human Touch + Ù…Ù†Ø§ÙØ³ÙŠÙ† + ÙØ­Øµ Ø¨Ø´Ø±ÙŠØ©")

# (Ù†ÙØ¨Ù‚ÙŠ Ø¹Ù„Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ÙƒÙ…Ø§ Ù‡ÙŠ Ù„Ø¯ÙŠÙƒ)
BASE_TMPL = read_prompt("base.md")
POLISH_TMPL = read_prompt("polish.md")
FAQ_TMPL = read_prompt("faq.md")
METH_TMPL = read_prompt("methodology.md")
CRITERIA_MAP = {
    "Ø¨ÙŠØªØ²Ø§": read_prompt("criteria_pizza.md"),
    "Ù…Ù†Ø¯ÙŠ": read_prompt("criteria_mandy.md"),
    "Ø¨Ø±Ø¬Ø±": read_prompt("criteria_burger.md"),
    "ÙƒØ§ÙÙŠÙ‡Ø§Øª": read_prompt("criteria_cafes.md"),
}
GENERAL_CRITERIA = read_prompt("criteria_general.md")

def _has_api_key() -> bool:
    try:
        if hasattr(st, "secrets") and "OPENAI_API_KEY" in st.secrets and st.secrets["OPENAI_API_KEY"]:
            return True
    except Exception:
        pass
    return bool(os.getenv("OPENAI_API_KEY"))

def slugify(name: str) -> str:
    s = ''.join(c for c in unicodedata.normalize('NFKD', name) if not unicodedata.combining(c))
    import re as _re
    s = _re.sub(r'\W+', '_', s).strip('_').lower()
    return s or "custom"

PLACE_TEMPLATES = {
    "Ù…ÙˆÙ„/Ù…Ø¬Ù…Ø¹": "Ø§Ø­Ø¬Ø² Ù‚Ø¨Ù„ Ø§Ù„Ø°Ø±ÙˆØ© Ø¨Ù€20â€“30 Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø±Ø§Ù‚Ø¨ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ø±ÙˆØ¶/Ø§Ù„Ù†Ø§ÙÙˆØ±Ø©ØŒ ÙˆØªØ¬Ù†Ù‘Ø¨ Ø·ÙˆØ§Ø¨ÙŠØ± Ø§Ù„Ù…ØµØ§Ø¹Ø¯.",
    "Ø¬Ù‡Ø© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© (Ø´Ù…Ø§Ù„/Ø´Ø±Ù‚..)": "Ø§Ù„ÙˆØµÙˆÙ„ Ø£Ø³Ù‡Ù„ Ø¹Ø¨Ø± Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ© Ù‚Ø¨Ù„ 7:30Ù…ØŒ Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ø´ÙˆØ§Ø±Ø¹ Ù‚Ø¯ ØªÙ…ØªÙ„Ø¦ Ù…Ø¨ÙƒØ±Ù‹Ø§ ÙÙŠ Ø§Ù„ÙˆÙŠÙƒÙ†Ø¯.",
    "Ø­ÙŠÙ‘ Ù…Ø­Ø¯Ø¯": "Ø§Ù„Ù…Ø´ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ø´Ø§Ø¡ Ø®ÙŠØ§Ø± Ù„Ø·ÙŠÙ Ø¥Ù† ØªÙˆÙÙ‘Ø±Øª Ø£Ø±ØµÙØ© Ù‡Ø§Ø¯Ø¦Ø©ØŒ Ø§Ù†ØªØ¨Ù‡ Ù„Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø°Ø±ÙˆØ© Ø¨ÙŠÙ† Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ§Ù„ÙˆÙŠÙƒÙ†Ø¯.",
    "Ø´Ø§Ø±Ø¹/Ù…Ù…Ø´Ù‰": "Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø£Ù„Ø·Ù Ø¨Ø¹Ø¯ Ø§Ù„Ù…ØºØ±Ø¨ ØµÙŠÙÙ‹Ø§ØŒ ÙˆØ§Ù„Ø¨Ø±Ø¯ Ø§Ù„Ù„ÙŠÙ„ÙŠ Ù‚Ø¯ ÙŠØªØ·Ù„Ù‘Ø¨ Ù…Ø´Ø±ÙˆØ¨Ù‹Ø§ Ø³Ø§Ø®Ù†Ù‹Ø§ Ø´ØªØ§Ø¡Ù‹.",
    "ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø­Ø±ÙŠØ©/ÙƒÙˆØ±Ù†ÙŠØ´": "Ø§Ù„Ù‡ÙˆØ§Ø¡ Ø£Ù‚ÙˆÙ‰ Ù…Ø³Ø§Ø¡Ù‹â€”Ø§Ø·Ù„Ø¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø³Ø±ÙŠØ¹Ù‹Ø§ ÙˆÙŠÙÙØ¶Ù‘Ù„ Ø§Ù„Ù…Ù‚Ø§Ø¹Ø¯ Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø© Ø¹Ù† Ø§Ù„ØªÙŠØ§Ø±Ø§Øª.",
    "ÙÙ†Ø¯Ù‚/Ù…Ù†ØªØ¬Ø¹": "Ù‚Ø¯ ØªØ±ØªÙØ¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù„ÙƒÙ† Ø§Ù„Ø®Ø¯Ù…Ø© Ø£Ø¯Ù‚Ù‘ØŒ Ø§Ø­Ø¬Ø² Ø¨Ø§ÙƒØ±Ù‹Ø§ Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ù†ÙˆØ§ÙØ°/Ø§Ù„Ø¥Ø·Ù„Ø§Ù„Ø§Øª.",
    "Ù…Ø¯ÙŠÙ†Ø© ÙƒØ§Ù…Ù„Ø©": "ÙØ±ÙˆØ¹ Ø³Ù„Ø³Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù‚Ø¯ ØªØ®ØªÙ„Ù Ø¬ÙˆØ¯ØªÙ‡Ø§ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ØŒ Ø§Ø·Ù„Ø¨ Ø§Ù„Ø·Ø¨Ù‚ Ø§Ù„Ø£Ø´Ù‡Ø± Ø£ÙˆÙ„Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªÙˆÙ‰."
}
def build_protip_hint(place_type: str) -> str:
    return PLACE_TEMPLATES.get(place_type or "", "Ù‚Ø¯Ù‘Ù… Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ù„Ø°Ø±ÙˆØ© ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ÙˆØµÙˆÙ„.")
def build_place_context(place_type: str, place_name: str, place_rules: str, strict: bool) -> str:
    scope = "ØµØ§Ø±Ù… (Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚ ÙÙ‚Ø·)" if strict else "Ù…Ø±Ù† (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚)"
    return f"""Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ÙƒØ§Ù†:
- Ø§Ù„Ù†ÙˆØ¹: {place_type or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"}
- Ø§Ù„Ø§Ø³Ù…: {place_name or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"}
- Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚: {place_rules or "â€”"}
- ØµØ±Ø§Ù…Ø© Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚: {scope}"""

# Sidebar
st.sidebar.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©")
tone = st.sidebar.selectbox(
    "Ù†ØºÙ…Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨",
    ["Ù†Ø§Ù‚Ø¯ ÙˆØ¯ÙˆØ¯", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù…", "Ø¯Ù„ÙŠÙ„ ØªØ­Ø±ÙŠØ±ÙŠ Ù…Ø­Ø§ÙŠØ¯", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª"]
)
primary_model = st.sidebar.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ", ["gpt-4.1", "gpt-4o", "gpt-4o-mini"], index=0)
fallback_model = st.sidebar.selectbox("Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¯ÙŠÙ„ (Fallback)", ["gpt-4o", "gpt-4o-mini", "gpt-4.1"], index=1)
include_faq = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© Ù‚Ø³Ù… FAQ", value=True)
include_methodology = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ø±ÙŠØ±", value=True)
add_human_touch = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ Ø·Ø¨Ù‚Ø© Ø§Ù„Ù„Ù…Ø³Ø§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (Polish)", value=True)
approx_len = st.sidebar.slider("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ (ÙƒÙ„Ù…Ø§Øª)", 600, 1800, 1100, step=100)
# (Ø§Ù„Ù‡Ø§Ù…Ø´ 12% Ø«Ø§Ø¨Øª â€” ÙŠÙ…ÙƒÙ† ØªØ±Ù‚ÙŠØ© Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ù† Ø£Ø±Ø¯Øª)

review_weight = None
if tone in ["Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª"]:
    default_weight = 85 if tone == "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±" else 55
    review_weight = st.sidebar.slider("ÙˆØ²Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª (Ùª)", 0, 100, default_weight, step=5)

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
internal_catalog = st.sidebar.text_area(
    "Ø£Ø¯Ø®Ù„ Ø¹Ù†Ø§ÙˆÙŠÙ†/Ø³Ù„Ø§Ú¯Ø² Ù…Ù‚Ø§Ù„Ø§ØªÙƒ (Ø³Ø·Ø± Ù„ÙƒÙ„ Ø¹Ù†ØµØ±)",
    "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ø±ÙŠØ§Ø¶\nØ£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¥ÙØ·Ø§Ø± ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶\nØ£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¨ÙŠØªØ²Ø§ ÙÙŠ Ø¬Ø¯Ø©"
)

# ====== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ======
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ—ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ (Ù„Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø³Ø±ÙŠØ¹)")

def _get_secret_fuzzy(primary: str, aliases: tuple[str, ...] = ()):
    names = (primary,) + aliases
    for nm in names:
        v = None
        try:
            if hasattr(st, "secrets"):
                v = st.secrets.get(nm)
        except Exception:
            v = None
        if not v:
            v = os.getenv(nm)
        if isinstance(v, str):
            v = v.strip()
        if v:
            return v
    try:
        all_keys = list(getattr(st, "secrets", {}).keys())
    except Exception:
        all_keys = []
    norm = lambda s: "".join(s.split()).lower() if isinstance(s, str) else ""
    target_set = {norm(nm) for nm in names}
    for k in all_keys:
        if norm(k) in target_set:
            v = st.secrets.get(k)
            if isinstance(v, str):
                v = v.strip()
            if v:
                return v
    return None

def _mask_value(val: str, show_last: int = 4) -> str:
    if not val:
        return "â€”"
    s = str(val)
    s_clean = "".join(s.split())
    if len(s_clean) <= show_last:
        return "â€¢" * len(s_clean)
    return "â€¢" * (len(s_clean) - show_last) + s_clean[-show_last:]

def _domain_from_url(u: str) -> str:
    if not u: return "â€”"
    try:
        netloc = urlparse(u).netloc
        return netloc or u
    except Exception:
        return u

st.sidebar.caption(
    f"WP_BASE_URL: {'OK' if _get_secret_fuzzy('WP_BASE_URL', ('WP_URL','WORDPRESS_BASE_URL')) else 'MISSING'} Â· "
    f"WP_USER: {'OK' if _get_secret_fuzzy('WP_USER', ('WORDPRESS_USER','WP_USERNAME')) else 'MISSING'} Â· "
    f"WP_APP_PASS: {'OK' if _get_secret_fuzzy('WP_APP_PASS', ('WP_APP_PASSWORD','WORDPRESS_APP_PASSWORD')) else 'MISSING'}"
)

reveal_wp = st.sidebar.checkbox("Ø¥Ø¸Ù‡Ø§Ø± Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ø¯ÙˆÙ…ÙŠÙ† (Ù„Ù† Ù†Ø¹Ø±Ø¶ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±)", value=False)
wp_base_sb = _get_secret_fuzzy("WP_BASE_URL", aliases=("WP_URL", "WORDPRESS_BASE_URL"))
wp_user_sb = _get_secret_fuzzy("WP_USER", aliases=("WORDPRESS_USER", "WP_USERNAME"))
wp_pass_sb = _get_secret_fuzzy("WP_APP_PASS", aliases=("WP_APP_PASSWORD", "WORDPRESS_APP_PASSWORD"))
st.sidebar.write("**Ø§Ù„Ù…Ø¶ÙŠÙ (Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†):** ", _domain_from_url(wp_base_sb) if reveal_wp else "Ù…Ø®ÙÙŠ")
st.sidebar.write("**Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** ", (wp_user_sb or "â€”") if reveal_wp else _mask_value(wp_user_sb or "", show_last=0))
st.sidebar.write("**ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚:** ", _mask_value(wp_pass_sb or ""))  # Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ù…ÙÙ‚Ù†Ù‘Ø¹Ø©
if reveal_wp and wp_base_sb:
    st.sidebar.markdown(f"[Ø§Ø®ØªØ¨Ø§Ø± REST]({wp_base_sb.rstrip('/')}/wp-json/)")

# Tabs (Ø£Ø¶ÙÙ†Ø§ ØªØ¨ÙˆÙŠØ¨ Google ÙƒÙ€ Ø±Ø§Ø¨Ø¹ ØªØ¨ÙˆÙŠØ¨)
tab_article, tab_comp, tab_qc, tab_places = st.tabs([
    "âœï¸ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„",
    "ğŸ†š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ† (Ø±ÙˆØ§Ø¨Ø· ÙŠØ¯ÙˆÙŠØ©)",
    "ğŸ§ª ÙØ­Øµ Ø¨Ø´Ø±ÙŠØ© ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
    "ğŸŒ Ø¬Ù„Ø¨ Ù…Ø·Ø§Ø¹Ù… Ù…Ù† Google"
])

# ------------------ Tab 1: Article Generation ------------------
with tab_article:
    col1, col2 = st.columns([2,1])
    with col1:
        article_title = st.text_input("Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„", "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶")
        keyword = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", "Ù…Ø·Ø§Ø¹Ù… ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶")

        COUNTRIES = {"Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©": ["Ø§Ù„Ø±ÙŠØ§Ø¶","Ø¬Ø¯Ø©","Ù…ÙƒØ©","Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©","Ø§Ù„Ø¯Ù…Ø§Ù…","Ø§Ù„Ø®Ø¨Ø±","Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†","Ø§Ù„Ø·Ø§Ø¦Ù","Ø£Ø¨Ù‡Ø§","Ø®Ù…ÙŠØ³ Ù…Ø´ÙŠØ·","Ø¬Ø§Ø²Ø§Ù†","Ù†Ø¬Ø±Ø§Ù†","ØªØ¨ÙˆÙƒ","Ø¨Ø±ÙŠØ¯Ø©","Ø¹Ù†ÙŠØ²Ø©","Ø§Ù„Ù‡ÙÙˆÙ","Ø§Ù„Ø£Ø­Ø³Ø§Ø¡","Ø§Ù„Ø¬Ø¨ÙŠÙ„","Ø§Ù„Ù‚Ø·ÙŠÙ","ÙŠÙ†Ø¨Ø¹","Ø­Ø§Ø¦Ù„"],
                     "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª": ["Ø¯Ø¨ÙŠ","Ø£Ø¨ÙˆØ¸Ø¨ÙŠ","Ø§Ù„Ø´Ø§Ø±Ù‚Ø©","Ø¹Ø¬Ù…Ø§Ù†","Ø±Ø£Ø³ Ø§Ù„Ø®ÙŠÙ…Ø©","Ø§Ù„ÙØ¬ÙŠØ±Ø©","Ø£Ù… Ø§Ù„Ù‚ÙŠÙˆÙŠÙ†","Ø§Ù„Ø¹ÙŠÙ†"]}
        country = st.selectbox("Ø§Ù„Ø¯ÙˆÙ„Ø©", ["Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©", "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª", "Ø£Ø®Ø±Ù‰â€¦"], index=0)
        if country in COUNTRIES:
            city_choice = st.selectbox("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", COUNTRIES[country] + ["Ù…Ø¯ÙŠÙ†Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦"], index=0)
            city_input = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", city_choice) if city_choice == "Ù…Ø¯ÙŠÙ†Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦" else city_choice
        else:
            country = st.text_input("Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆÙ„Ø©", "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©")
            city_input = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", "Ø§Ù„Ø±ÙŠØ§Ø¶")

        place_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ù…ÙƒØ§Ù†",
            ["Ù…ÙˆÙ„/Ù…Ø¬Ù…Ø¹", "Ø¬Ù‡Ø© Ù…Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© (Ø´Ù…Ø§Ù„/Ø´Ø±Ù‚..)", "Ø­ÙŠÙ‘ Ù…Ø­Ø¯Ø¯", "Ø´Ø§Ø±Ø¹/Ù…Ù…Ø´Ù‰", "ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø­Ø±ÙŠØ©/ÙƒÙˆØ±Ù†ÙŠØ´", "ÙÙ†Ø¯Ù‚/Ù…Ù†ØªØ¬Ø¹", "Ù…Ø¯ÙŠÙ†Ø© ÙƒØ§Ù…Ù„Ø©"], index=0)
        place_name = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…ÙƒØ§Ù†/Ø§Ù„Ù†Ø·Ø§Ù‚", placeholder="Ù…Ø«Ù„Ù‹Ø§: Ø¯Ø¨ÙŠ Ù…ÙˆÙ„ / Ø´Ù…Ø§Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶")
        place_rules = st.text_area("Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", placeholder="Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙˆÙ„ ÙÙ‚Ø·ØŒ Ø£Ùˆ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡: Ø§Ù„Ø±Ø¨ÙŠØ¹/Ø§Ù„ÙŠØ§Ø³Ù…ÙŠÙ†/Ø§Ù„Ù…Ø±ÙˆØ¬â€¦", height=80)
        strict_in_scope = st.checkbox("Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ ÙÙ‚Ø· (ØµØ§Ø±Ù…)", value=True)

        content_scope = st.radio("Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­ØªÙˆÙ‰", ["ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØ§Ù†", "Ø´Ø§Ù…Ù„ Ø¨Ù„Ø§ ÙØ¦Ø©", "Ù‡Ø¬ÙŠÙ† (ØªÙ‚Ø³ÙŠÙ… Ø¯Ø§Ø®Ù„ÙŠ)"], index=1 if place_type=="Ù…ÙˆÙ„/Ù…Ø¬Ù…Ø¹" else 0)

        built_in_labels = list(CRITERIA_MAP.keys())
        category = "Ø¹Ø§Ù…"
        criteria_block = GENERAL_CRITERIA

        # ---------------- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø© ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„ÙŠ + Ø¹Ù„Ø§Ù…Ø© Ù‡Ù„ Ù‡ÙŠ Ù…Ø®ØµÙ‘ØµØ© ----------------
        is_custom_category = False
        if content_scope == "ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØ§Ù†":
            category_choice = st.selectbox("Ø§Ù„ÙØ¦Ø©", built_in_labels + ["ÙØ¦Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦"])

            if category_choice == "ÙØ¦Ø© Ù…Ø®ØµÙ‘ØµØ©â€¦":
                # Ø­Ù‚Ù† Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¹Ù„Ù‘Ù‚Ø© (Ø¥Ù† ÙˆÙØ¬Ø¯Øª) Ù‚Ø¨Ù„ Ø¥Ù†Ø´Ø§Ø¡ Text Area
                if "pending_custom_criteria_text" in st.session_state:
                    st.session_state["custom_criteria_text"] = st.session_state.pop("pending_custom_criteria_text")

                custom_category_name = st.text_input("Ø§Ø³Ù… Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø®ØµÙ‘ØµØ©", "Ù…Ø·Ø§Ø¹Ù… Ù„Ø¨Ù†Ø§Ù†ÙŠØ©", key="custom_category_name")

                # Ù„Ø§ Ù†Ù…Ø±Ù‘Ø± value Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§Ø› ÙÙ‚Ø· Ø£ÙˆÙ„ ØªØ´ØºÙŠÙ„
                DEFAULT_CRIT_MD = (
                    "- **Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©:** Ø²ÙŠØ§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ù‘Ø¯Ø© ÙˆØªØ¬Ø±Ø¨Ø© Ø£Ø·Ø¨Ø§Ù‚ Ø£Ø³Ø§Ø³ÙŠØ© ÙˆÙ…Ø¹Ø±ÙˆÙØ© ÙÙŠ Ø§Ù„Ù…Ø·Ø¨Ø®.\n"
                    "- **Ø§Ù„Ù…ÙƒÙˆÙ‘Ù†Ø§Øª:** Ø¬ÙˆØ¯Ø© Ø§Ù„Ù„Ø­ÙˆÙ…/Ø§Ù„Ø£Ø³Ù…Ø§Ùƒ/Ø§Ù„Ø£Ø¬Ø¨Ø§Ù† ÙˆØ§Ù„Ø®Ø¶Ø±ÙˆØ§Øª Ø§Ù„Ø·Ø§Ø²Ø¬Ø©.\n"
                    "- **Ø·Ø±Ù‚ Ø§Ù„Ø·Ù‡ÙŠ ÙˆØ§Ù„Ø£ØµØ§Ù„Ø©:** Ø§Ù„ØªØªØ¨ÙŠÙ„ ÙˆØ§Ù„ØªØ­Ù…ÙŠØ±/Ø§Ù„Ø´ÙˆÙŠ/Ø§Ù„ÙØ±Ù† ÙˆÙ…Ø¯Ù‰ Ø§Ù‚ØªØ±Ø§Ø¨ Ø§Ù„Ù†ÙƒÙ‡Ø© Ù…Ù† Ø§Ù„Ø£ØµÙ„.\n"
                    "- **Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡ ÙˆØ§Ù„Ù…Ù„Ø§Ø¡Ù…Ø©:** Ø¬Ù„Ø³Ø§Øª Ø¹Ø§Ø¦Ù„ÙŠØ©/Ø£ØµØ¯Ù‚Ø§Ø¡ØŒ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¶Ø¬ÙŠØ¬ ÙˆØ±Ø§Ø­Ø© Ø§Ù„Ø¬Ù„Ø³Ø§Øª.\n"
                    "- **Ø«Ø¨Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©:** Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ…Ø§Ø³Ùƒ ÙÙŠ Ø§Ù„Ø·Ø¹Ù… ÙˆØ§Ù„Ø®Ø¯Ù…Ø© Ø¹Ø¨Ø± Ø²ÙŠØ§Ø±Ø§Øª ÙˆØ£ÙˆÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©."
                )
                ta_kwargs = dict(key="custom_criteria_text", height=140)
                if "custom_criteria_text" not in st.session_state:
                    ta_kwargs["value"] = DEFAULT_CRIT_MD

                custom_criteria_text = st.text_area(
                    "Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø© (ÙŠØ¯ÙˆÙŠ â€” Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ø¬Ù„Ø¨)",
                    **ta_kwargs
                )

                category = (st.session_state.get("custom_category_name") or "ÙØ¦Ø© Ù…Ø®ØµÙ‘ØµØ©").strip()
                criteria_block = st.session_state.get("custom_criteria_text") or "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§ØªØŒ ØªÙ†ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ ÙˆØ«Ø¨Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©."
                is_custom_category = True
            else:
                category = category_choice
                criteria_block = CRITERIA_MAP.get(category_choice, GENERAL_CRITERIA)
                is_custom_category = False
        else:
            category = "Ø¹Ø§Ù…"
            criteria_block = GENERAL_CRITERIA
            is_custom_category = False
        # ---------------------------------------------------------------------

        # ---------- Ø¯ÙˆØ§Ù„ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ø±Ø¶ + Ø²Ø±/Ø®ÙŠØ§Ø± Ø¬Ù„Ø¨/ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ÙØ¦Ø© ----------
        def _normalize_criteria(raw):
            """Ø­ÙˆÙ‘Ù„ Ø£ÙŠ Ù†Ø§ØªØ¬ (list/tuple/dict/str JSON) Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù†ØµÙˆØµ Ù†Ø¸ÙŠÙØ© Ø¨Ù„Ø§ undefined."""
            if raw is None:
                return []
            if isinstance(raw, str):
                s = raw.strip()
                if s.startswith(("[", "{")):
                    try:
                        raw = json.loads(s)
                    except Exception:
                        lines = [ln.strip(" -â€¢\t").strip() for ln in s.splitlines() if ln.strip()]
                        return [ln for ln in lines if ln and ln.lower() != "undefined"]
                else:
                    lines = [ln.strip(" -â€¢\t").strip() for ln in s.splitlines() if ln.strip()]
                    return [ln for ln in lines if ln and ln.lower() != "undefined"]
            if isinstance(raw, dict):
                for k in ("criteria", "bullets", "items", "list"):
                    if k in raw:
                        raw = raw[k]
                        break
                else:
                    vals = list(raw.values())
                    raw = vals if all(isinstance(v, str) for v in vals) else list(raw.keys())
            if isinstance(raw, (list, tuple)):
                out = []
                for x in raw:
                    if isinstance(x, str):
                        t = x.strip().strip(",").strip('"').strip("'")
                    elif isinstance(x, dict) and "text" in x:
                        t = str(x["text"]).strip()
                    else:
                        t = str(x).strip()
                    if t and t.lower() != "undefined":
                        out.append(t)
                return out
            return [str(raw)]

        def _format_criteria_md(items):
            items = _normalize_criteria(items)
            return "\n".join(f"- {c}" for c in items) or "- â€”"

        effective_category = (category or "Ø¹Ø§Ù…").strip()
        if "criteria_generated_md_map" not in st.session_state:
            st.session_state["criteria_generated_md_map"] = {}

        with st.expander("ğŸ“‹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø© (ØªÙ„Ù‚Ø§Ø¦ÙŠ/ÙŠØ¯ÙˆÙŠ)", expanded=False):
            st.caption(f"Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: **{effective_category}**")
            use_llm = st.checkbox("ØªØ¹Ø²ÙŠØ² Ø¨Ø§Ù„Ù€ LLM (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", value=False, key="crit_llm",
                                  help="ÙŠØªØ·Ù„Ø¨ OPENAI_API_KEY Ø¥Ù† ÙØ¹Ù‘Ù„ØªÙ‡ØŒ ÙˆØ¥Ù„Ø§ ØªÙØ³ØªØ®Ø¯Ù… Heuristics.")
            if st.button("Ø¬Ù„Ø¨/ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ÙØ¦Ø©", key="btn_generate_criteria"):
                crit_list = get_category_criteria(
                    effective_category,
                    use_llm=use_llm,
                    catalog_path="data/criteria_catalog.yaml"
                )
                md_ = _format_criteria_md(crit_list)
                st.session_state["criteria_generated_md_map"].pop(effective_category, None)
                st.session_state["criteria_generated_md_map"][effective_category] = md_

                if is_custom_category:
                    st.session_state["pending_custom_criteria_text"] = md_
                    if getattr(st, "rerun", None):
                        st.rerun()
                    else:
                        st.experimental_rerun()
                else:
                    st.success("ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ÙˆØ­ÙØ¸Ù‡Ø§.")

            if effective_category in st.session_state["criteria_generated_md_map"]:
                st.markdown("**Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± (ØªÙ„Ù‚Ø§Ø¦ÙŠ):**")
                st.markdown(st.session_state["criteria_generated_md_map"][effective_category])

        if is_custom_category:
            criteria_block = st.session_state.get("custom_criteria_text", criteria_block)
        else:
            criteria_block = st.session_state.get("criteria_generated_md_map", {}).get(effective_category, criteria_block)

        restaurants_input = st.text_area(
            "Ø£Ø¯Ø®Ù„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (Ø³Ø·Ø± Ù„ÙƒÙ„ Ù…Ø·Ø¹Ù…)",
            st.session_state.get("restaurants_text", "Ù…Ø·Ø¹Ù… 1\nÙ…Ø·Ø¹Ù… 2\nÙ…Ø·Ø¹Ù… 3"),
            height=160
        )
        st.markdown("**Ø£Ùˆ** Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (Ø¹Ù…ÙˆØ¯: name)")
        csv_file = st.file_uploader("Ø±ÙØ¹ CSV (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", type=["csv"], help="Ø¹Ù…ÙˆØ¯ name Ù…Ø·Ù„ÙˆØ¨Ø› Ø¹Ù…ÙˆØ¯ note Ø§Ø®ØªÙŠØ§Ø±ÙŠ.")

        def _normalize_name(s: str) -> str:
            return " ".join((s or "").strip().split())
        def _merge_unique(a: list, b: list) -> list:
            seen, out = set(), []
            for x in a + b:
                x2 = _normalize_name(x)
                if x2 and x2 not in seen:
                    seen.add(x2); out.append(x2)
            return out

        typed_restaurants = [r.strip() for r in restaurants_input.splitlines() if r.strip()]
        uploaded_restaurants = []
        if csv_file:
            try:
                text = csv_file.read().decode("utf-8-sig")
                reader = csv.DictReader(io.StringIO(text))
                for row in reader:
                    name = row.get("name") or row.get("Ø§Ø³Ù…") or ""
                    if name.strip():
                        uploaded_restaurants.append(name.strip())
            except Exception as e:
                st.warning(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© CSV: {e}")
        restaurants = _merge_unique(typed_restaurants, uploaded_restaurants)

        manual_notes = st.text_area("Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙŠØ¯ÙˆÙŠØ© ØªÙØ¯Ù…Ø¬ Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", st.session_state.get("comp_gap_notes",""))

    with col2:
        st.subheader("Ù‚Ø§Ø¦Ù…Ø© ØªØ¯Ù‚ÙŠÙ‚ Ø¨Ø´Ø±ÙŠØ©")
        checks = {
            "sensory": st.checkbox("Ø£Ø¶Ù ÙˆØµÙÙ‹Ø§ Ø­Ø³ÙŠÙ‹Ø§ Ø¯Ù‚ÙŠÙ‚Ù‹Ø§ (Ø±Ø§Ø¦Ø­Ø©/Ù‚ÙˆØ§Ù…/Ø­Ø±Ø§Ø±Ø©) Ù„Ù…Ø·Ø¹Ù… ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„"),
            "personal": st.checkbox("Ø£Ø¯Ø±Ø¬ Ù…Ù„Ø§Ø­Ø¸Ø© Ø´Ø®ØµÙŠØ©/ØªÙØ¶ÙŠÙ„ Ø´Ø®ØµÙŠ"),
            "compare": st.checkbox("Ø£Ø¶Ù Ù…Ù‚Ø§Ø±Ù†Ø© ØµØºÙŠØ±Ø© Ù…Ø¹ Ø²ÙŠØ§Ø±Ø© Ø³Ø§Ø¨Ù‚Ø©/Ù…Ø·Ø¹Ù… Ù…Ø´Ø§Ø¨Ù‡"),
            "critique": st.checkbox("Ø£Ø¶Ù Ù†Ù‚Ø¯Ù‹Ø§ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ (ØªÙØµÙŠÙ„Ø© Ø³Ù„Ø¨ÙŠØ© ØµØºÙŠØ±Ø©)"),
            "vary": st.checkbox("Ù†ÙˆÙ‘Ø¹ Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø±ØªØ§Ø¨Ø©"),
        }

    if st.button("ğŸš€ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„"):
        if not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
            st.stop()
        client = get_client()

        if tone == "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±":
            tone_instructions = ("Ø§ÙƒØªØ¨ ÙƒÙ†Ù‘Ø§Ù‚Ø¯ ØµØ§Ø±Ù… ÙŠØ¹ØªÙ…Ø¯ Ø£Ø³Ø§Ø³Ù‹Ø§ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø© Ø¹Ù„Ù†Ù‹Ø§. "
                                 "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© ÙˆØ§Ø°ÙƒØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©. Ù„Ø§ ØªØ¯Ù‘Ø¹Ù visita Ø´Ø®ØµÙŠØ©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£Ø±Ù‚Ø§Ù….")
            tone_selection_line = "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© Ù…Ù†Ø´ÙˆØ±Ø© Ø¹Ù„Ù†Ù‹Ø§ Ø­ØªÙ‰ {last_updated}ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©."
            system_tone = "Ø£Ø³Ù„ÙˆØ¨ Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… Ù…Ø±ØªÙƒØ² Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±"
        elif tone == "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª":
            tone_instructions = ("Ø§ÙƒØªØ¨ ÙƒÙ†Ù‘Ø§Ù‚Ø¯ ØµØ§Ø±Ù… ÙŠÙ…Ø²Ø¬ Ø®Ø¨Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ© Ù…Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±. "
                                 "Ù‚Ø¯Ù‘Ù… Ø§Ù„Ø­ÙƒÙ… Ù…Ù† Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø£ÙˆÙ„Ù‹Ø§ Ø«Ù… Ù‚Ø§Ø±Ù†Ù‡ Ø¨Ø§Ù†Ø·Ø¨Ø§Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±. Ø£Ø¯Ø±Ø¬ **Ù†Ù‚Ø·Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†** Ù„ÙƒÙ„ Ù…Ø·Ø¹Ù….")
            tone_selection_line = "Ù…Ø²Ø¬Ù†Ø§ Ø¨ÙŠÙ† Ø²ÙŠØ§Ø±Ø§Øª Ù…ÙŠØ¯Ø§Ù†ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨ ÙØ¹Ù„ÙŠØ© ÙˆÙ…Ø±Ø§Ø¬Ø¹Ø§Øª Ø¹Ø§Ù…Ø© Ø­ØªÙ‰ {last_updated}."
            system_tone = "Ø£Ø³Ù„ÙˆØ¨ Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… ÙŠÙ…Ø²Ø¬ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±"
        else:
            tone_instructions = "Ø§ÙƒØªØ¨ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…ØªÙˆØ§Ø²Ù† ÙŠØ±Ø§Ø¹ÙŠ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ÙˆØ¶ÙˆØ­ Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ©."
            tone_selection_line = "Ø§Ø¹ØªÙ…Ø¯Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© Ù…ØªØ§Ø­Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯ÙˆØ±ÙŠØ©."
            system_tone = tone

        if content_scope == "ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØ§Ù†":
            scope_instructions = "Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ."
        elif content_scope == "Ù‡Ø¬ÙŠÙ† (ØªÙ‚Ø³ÙŠÙ… Ø¯Ø§Ø®Ù„ÙŠ)":
            scope_instructions = "Ù‚Ø³Ù‘Ù… Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙˆØ§Ø²Ù† Ø§Ù„ØªÙ†ÙˆØ¹."
        else:
            scope_instructions = "Ù‚Ø¯Ù‘Ù… ØªØ´ÙƒÙŠÙ„Ø© Ù…ØªÙ†ÙˆØ¹Ø© ØªÙ…Ø«Ù‘Ù„ Ø§Ù„Ù…ÙƒØ§Ù†."

        protip_hint = build_protip_hint(place_type)
        place_context = build_place_context(place_type, place_name, place_rules, strict_in_scope)

        faq_block = FAQ_TMPL.format(category=category, city=place_name or city_input) if include_faq else "â€”"
        last_updated = datetime.now().strftime("%B %Y")
        methodology_block = METH_TMPL.format(last_updated=last_updated) if include_methodology else "â€”"

        base_prompt = BASE_TMPL.format(
            title=article_title, keyword=keyword, content_scope=content_scope, category=category,
            restaurants_list=", ".join(restaurants), criteria_block=criteria_block, faq_block=faq_block,
            methodology_block=methodology_block, tone_label=tone, place_context=place_context,
            protip_hint=protip_hint, scope_instructions=scope_instructions, tone_instructions=tone_instructions,
            tone_selection_line=tone_selection_line.replace("{last_updated}", last_updated)
        )
        base_messages = [
            {"role": "system", "content": f"Ø§ÙƒØªØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰. {system_tone}. Ø·ÙˆÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ {approx_len} ÙƒÙ„Ù…Ø©."},
            {"role": "user", "content": base_prompt},
        ]
        try:
            article_md = chat_complete(client, base_messages, max_tokens=2200, temperature=0.7, model=primary_model, fallback_model=fallback_model)
        except Exception as e:
            st.error(f"ÙØ´Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
            st.stop()

        # --- Ø·Ø¨Ù‚Ø© Polish (ÙƒÙ…Ø§ ÙÙŠ Ù†Ø³Ø®ØªÙƒ) ---
        apply_polish = add_human_touch or any(checks.values())
        merged_user_notes = (st.session_state.get("comp_gap_notes","") + "\n" + (manual_notes or "")).strip()
        if apply_polish or merged_user_notes:
            polish_prompt = read_prompt("polish.md").format(article=article_md, user_notes=merged_user_notes)
            polish_messages = [
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ Ù…Ø­ØªØ±ÙØŒ ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ ÙˆØªØ¶ÙŠÙ Ù„Ù…Ø³Ø§Øª Ø¨Ø´Ø±ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ©."},
                {"role": "user", "content": polish_prompt},
            ]
            try:
                article_md = chat_complete(client, polish_messages, max_tokens=2400, temperature=0.8, model=primary_model, fallback_model=fallback_model)
            except Exception as e:
                st.warning(f"Ø·Ø¨Ù‚Ø© Ø§Ù„Ù„Ù…Ø³Ø§Øª Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ØªØ¹Ø°Ù‘Ø±Øª: {e}")

        # --- âœ… ÙØ±Ø¶ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ù„Ù„Ù…Ø³Ø§Øª ÙˆØ§Ù„Ø·ÙˆÙ„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ---
        # 1) Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ FAQ ÙˆÙ…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªØ­Ø±ÙŠØ± Ø¥Ù† ØªÙ… ØªÙØ¹ÙŠÙ„Ù‡Ù…Ø§
        article_md = ensure_sections(article_md, need_faq=include_faq, need_meth=include_methodology)

        # 2) fallback Ù„Ù…Ø³Ø§Øª Ø¨Ø´Ø±ÙŠØ© Ø¥Ù† Ø§Ø®ØªØ±ØªÙ‡Ø§ ÙˆÙØ´Ù„ Ø§Ù„Ù€ LLM ÙÙŠ ØªØ­Ø³ÙŠÙ† ÙˆØ§Ø¶Ø­
        if add_human_touch:
            article_md = add_human_fallback(article_md)

        # 3) Ø¶Ø¨Ø· Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ Â±12%
        article_md = enforce_word_target_via_llm(
            client, primary_model, fallback_model, article_md, target_words=int(approx_len), tolerance_pct=12
        )

        # ğŸ” Ø­Ù‚Ù† Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø­Ù…ÙŠØ© 100% ØªØ­Øª ÙƒÙ„ H3 Ù‚Ø¨Ù„ Ø§Ù„Meta/Links
        if "places_index" in st.session_state and st.session_state["places_index"]:
            article_md = inject_details_under_h3(article_md, st.session_state["places_index"])

        meta_prompt = f"ØµÙØº Ø¹Ù†ÙˆØ§Ù† SEO (â‰¤ 60) ÙˆÙˆØµÙ Ù…ÙŠØªØ§ (â‰¤ 155) Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ù†ÙˆØ§Ù† \"{article_title}\". Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {keyword}.\nTITLE: ...\nDESCRIPTION: ..."
        try:
            meta_out = chat_complete(client, [{"role":"system","content":"Ø£Ù†Øª Ù…Ø®ØªØµ SEO Ø¹Ø±Ø¨ÙŠ."},{"role":"user","content": meta_prompt}], max_tokens=200, temperature=0.6, model=primary_model, fallback_model=fallback_model)
        except Exception:
            meta_out = f"TITLE: {article_title}\nDESCRIPTION: Ø¯Ù„ÙŠÙ„ Ø¹Ù…Ù„ÙŠ Ø¹Ù† {keyword}."

        links_catalog = [s.strip() for s in internal_catalog.splitlines() if s.strip()]
        links_prompt = f"Ø§Ù‚ØªØ±Ø­ 3 Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù† Ø£Ù…ÙƒÙ†:\n{links_catalog}\nØ§Ù„Ø¹Ù†ÙˆØ§Ù†: {article_title}\nØ§Ù„Ù†Ø·Ø§Ù‚: {content_scope}\nØ§Ù„ÙØ¦Ø©: {category}\nØ§Ù„Ù…Ø¯ÙŠÙ†Ø©/Ø§Ù„Ù…ÙƒØ§Ù†: {place_name or city_input}\nÙ…Ù‚ØªØ·Ù:\n{article_md[:800]}\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: <Ø§Ù„Ù†Øµ>\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: <Ø§Ù„Ù†Øµ>\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: <Ø§Ù„Ù†Øµ>"
        try:
            links_out = chat_complete(client, [{"role":"system","content":"Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ ÙŠÙ‚ØªØ±Ø­ Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ©."},{"role":"user","content": links_prompt}], max_tokens=240, temperature=0.5, model=primary_model, fallback_model=fallback_model)
        except Exception:
            links_out = "- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ø±ÙŠØ§Ø¶\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: Ø¯Ù„ÙŠÙ„ Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ø¹Ø§Ø¦Ù„Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶\n- Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ Ù…Ù‚ØªØ±Ø­: Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†Ù…Ø§Ø·"

        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ø§ØªØ¬")
        st.markdown(article_md)
        st.session_state['last_article_md'] = article_md
        st.session_state['last_title'] = article_title  # Ù„Ø­Ø³Ø§Ø¨ slug Ù„Ù„Ù†Ø´Ø±

        st.subheader("ğŸ” Meta (SEO)"); st.code(meta_out, language="text")
        st.subheader("ğŸ”— Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©"); st.markdown(links_out)

        json_obj = {"title": article_title, "keyword": keyword, "category": category,
            "country": country, "city": city_input, "place": {"type": place_type, "name": place_name, "rules": place_rules, "strict": strict_in_scope},
            "content_scope": content_scope, "restaurants": restaurants, "last_updated": datetime.now().strftime("%B %Y"),
            "tone": tone, "reviews_weight": review_weight, "models": {"primary": primary_model, "fallback": fallback_model},
            "include_faq": include_faq, "include_methodology": include_methodology,
            "article_markdown": article_md, "meta": meta_out, "internal_links": links_out}
        st.session_state['last_json'] = to_json(json_obj)

    with col2:
        colA, colB, colC = st.columns(3)
        with colA:
            md_data = st.session_state.get('last_article_md', '')
            st.download_button('ğŸ’¾ ØªÙ†Ø²ÙŠÙ„ Markdown', data=md_data, file_name='article.md', mime='text/markdown')
        with colB:
            md_data = st.session_state.get('last_article_md', '')
            st.download_button('ğŸ“ ØªÙ†Ø²ÙŠÙ„ DOCX', data=to_docx(md_data), file_name='article.docx', mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        with colC:
            json_data = st.session_state.get('last_json', '{}')
            st.download_button('ğŸ§© ØªÙ†Ø²ÙŠÙ„ JSON', data=json_data, file_name='article.json', mime='application/json')

    # ==== Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ (Ù…Ø¹ ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…ÙØ§ØªÙŠØ­) ====
    st.markdown("---")
    st.subheader("ğŸ“° Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³")

    wp_base = _get_secret_fuzzy("WP_BASE_URL", aliases=("WP_URL", "WORDPRESS_BASE_URL"))
    wp_user = _get_secret_fuzzy("WP_USER", aliases=("WORDPRESS_USER", "WP_USERNAME"))
    wp_pass = _get_secret_fuzzy("WP_APP_PASS", aliases=("WP_APP_PASSWORD", "WORDPRESS_APP_PASSWORD"))
    wp_ready = all([wp_base, wp_user, wp_pass])

    try:
        detected_keys = ", ".join(sorted(list(getattr(st, "secrets", {}).keys())))
    except Exception:
        detected_keys = "(Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© st.secrets)"
    st.caption("ğŸ” Ù…ÙØ§ØªÙŠØ­ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ st.secrets: " + detected_keys)
    st.caption(
        f"WP_BASE_URL: {'OK' if wp_base else 'MISSING'} Â· "
        f"WP_USER: {'OK' if wp_user else 'MISSING'} Â· "
        f"WP_APP_PASS: {'OK' if wp_pass else 'MISSING'}"
    )

    def slugify(name: str) -> str:
        s = ''.join(c for c in unicodedata.normalize('NFKD', name) if not unicodedata.combining(c))
        import re as _re
        s = _re.sub(r'\W+', '_', s).strip('_').lower()
        return s or "custom"

    current_title = st.session_state.get("last_title") or ""
    default_slug = slugify(current_title) if current_title else slugify(st.session_state.get('last_article_md', '')[:40] or "article")

    pcol1, pcol2 = st.columns([2,1])
    with pcol1:
        wp_slug = st.text_input("Slug (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", default_slug)
        wp_status = st.selectbox("Ø§Ù„Ø­Ø§Ù„Ø©", ["draft", "pending", "publish"], index=0)
    with pcol2:
        cattxt = st.text_input("IDs Ù„Ù„ØªØµÙ†ÙŠÙØ§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)", "")
        tagtxt = st.text_input("IDs Ù„Ù„ÙˆØ³ÙˆÙ… (Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„)", "")

    def wp_publish_draft(title: str, markdown_body: str, slug: str = None,
                         categories=None, tags=None, status: str = "draft") -> dict:
        base = (_get_secret_fuzzy("WP_BASE_URL", ("WP_URL","WORDPRESS_BASE_URL")) or "").rstrip("/")
        user = _get_secret_fuzzy("WP_USER", ("WORDPRESS_USER","WP_USERNAME"))
        app_pass = _get_secret_fuzzy("WP_APP_PASS", ("WP_APP_PASSWORD","WORDPRESS_APP_PASSWORD"))
        if not base or not user or not app_pass:
            raise RuntimeError("Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ Ù†Ø§Ù‚ØµØ©: WP_BASE_URL / WP_USER / WP_APP_PASS")
        html = md.markdown(markdown_body or "", extensions=["extra", "sane_lists"])
        url = f"{base}/wp-json/wp/v2/posts"
        payload = {"title": title or "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†", "content": html, "status": status}
        if slug: payload["slug"] = slug
        if categories: payload["categories"] = categories
        if tags: payload["tags"] = tags
        resp = requests.post(url, json=payload, auth=(user, app_pass), timeout=45)
        resp.raise_for_status()
        return resp.json()

    if not wp_ready:
        st.info("Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù†Ø§Ù‚ØµØ© Ø£Ùˆ ÙØ§Ø±ØºØ©. Ø£Ø¶Ù WP_BASE_URL Ùˆ WP_USER Ùˆ WP_APP_PASS Ø¥Ù„Ù‰ secrets.toml Ø«Ù… Ø§Ø¶ØºØ· Restart.")
    else:
        if st.button("ğŸš€ Ù†Ø´Ø± ÙƒÙ…Ø³ÙˆØ¯Ø© Ø¹Ù„Ù‰ ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³"):
            article_md_to_publish = st.session_state.get('last_article_md', '')
            if not article_md_to_publish.strip():
                st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù…Ù‚Ø§Ù„ Ù„Ù†Ø´Ø±Ù‡. Ø£Ù†Ø´Ø¦ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø£ÙˆÙ„Ù‹Ø§.")
            else:
                try:
                    cats = [int(x) for x in cattxt.split(",") if x.strip().isdigit()] if cattxt.strip() else None
                    tags = [int(x) for x in tagtxt.split(",") if x.strip().isdigit()] if tagtxt.strip() else None
                    res = wp_publish_draft(
                        title=current_title or "Ù…Ù‚Ø§Ù„ Ø¬Ø¯ÙŠØ¯",
                        markdown_body=article_md_to_publish,
                        slug=wp_slug or None,
                        categories=cats,
                        tags=tags,
                        status=wp_status,
                    )
                    st.success(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù†Ø´ÙˆØ± (ID={res.get('id')}) Ø¨Ø­Ø§Ù„Ø© {res.get('status')}.")
                    link = res.get("link") or (res.get("guid") or {}).get("rendered")
                    if link:
                        st.markdown(f"[ÙØªØ­ ÙÙŠ ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³]({link})")
                except Exception as e:
                    st.error(f"ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø±: {e}")

# ------------------ Tab 2: Competitor Analysis ------------------
with tab_comp:
    st.subheader("ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ Ù…Ù†Ø§ÙØ³ÙŠÙ† â€” Ø±ÙˆØ§Ø¨Ø· ÙŠØ¯ÙˆÙŠØ© (Ø¨Ø¯ÙˆÙ† API)")
    st.markdown("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø·ÙŠÙ† Ù„Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØµØ¯Ù‘Ø±Ø©. Ø³Ù†Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆÙ†Ø­Ù„Ù‘Ù„Ù‡ Ù…Ù† Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆE-E-A-T ÙÙ‚Ø·.")
    query = st.text_input("Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«", "Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¹Ù… Ø¯Ø¨ÙŠ Ù…ÙˆÙ„")
    place_scope_desc = st.text_input("ÙˆØµÙ Ø§Ù„Ù†Ø·Ø§Ù‚/Ø§Ù„Ù…ÙƒØ§Ù† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", "Ø¯Ø§Ø®Ù„ Ø¯Ø¨ÙŠ Ù…ÙˆÙ„ ÙÙ‚Ø·")
    url_a = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ A", "")
    url_b = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ B", "")

    tone_for_analysis = st.selectbox("Ù†Ø¨Ø±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„",
        ["Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±", "Ù†Ø§Ù‚Ø¯ ØµØ§Ø±Ù… | ØªØ¬Ø±Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© + Ù…Ø±Ø§Ø¬Ø¹Ø§Øª", "Ø¯Ù„ÙŠÙ„ ØªØ­Ø±ÙŠØ±ÙŠ Ù…Ø­Ø§ÙŠØ¯"], index=0)
    reviews_weight_analysis = st.slider("ÙˆØ²Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª (Ùª) ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„", 0, 100, 60, step=5)

    colx, coly = st.columns(2)
    with colx: fetch_btn = st.button("ğŸ“¥ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
    with coly: analyze_btn = st.button("ğŸ§  ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„")

    if fetch_btn:
        if not url_a or not url_b:
            st.warning("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø·ÙŠÙ† Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            try:
                with st.spinner("Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© A..."):
                    page_a = fetch_and_extract(url_a)
                with st.spinner("Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© B..."):
                    page_b = fetch_and_extract(url_b)
                st.session_state["comp_pages"] = {"A": page_a, "B": page_b}
                st.success("ØªÙ… Ø§Ù„Ø¬Ù„Ø¨ ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø©.")
                st.write("**A:**", page_a.get("title") or url_a, f"({page_a['word_count']} ÙƒÙ„Ù…Ø©)")
                st.write("**B:**", page_b.get("title") or url_b, f"({page_b['word_count']} ÙƒÙ„Ù…Ø©)")
                st.caption("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„.")
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± Ø§Ù„Ø¬Ù„Ø¨: {e}")

    if analyze_btn:
        if not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
            st.stop()
        pages = st.session_state.get("comp_pages")
        if not pages:
            st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            client = get_client()
            try:
                with st.spinner("ÙŠØ´ØºÙ‘Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
                    analysis_md = analyze_competitors(client, primary_model, fallback_model, pages["A"], pages["B"], query, place_scope_desc or "â€”", tone_for_analysis, reviews_weight_analysis)
                st.session_state["comp_analysis_md"] = analysis_md
                st.subheader("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„"); st.markdown(analysis_md)
                gaps = extract_gap_points(analysis_md)
                if gaps:
                    st.info("ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØµÙŠØ§Øª Gap-to-Win â€” ÙŠÙ…ÙƒÙ†Ùƒ Ø­Ù‚Ù†Ù‡Ø§ ÙÙŠ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…Ù‚Ø§Ù„.")
                    st.text_area("Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© (Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­Ø±ÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„Ø­Ù‚Ù†)", gaps, key="comp_gap_notes", height=160)
                else:
                    st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø³Ù… 'Gap-to-Win'. Ø§Ù†Ø³Ø®Ù‡ ÙŠØ¯ÙˆÙŠÙ‹Ø§.")
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")

# ------------------ Tab 3: QC ------------------
with tab_qc:
    st.subheader("ğŸ§ª ÙØ­Øµ Ø¨Ø´Ø±ÙŠØ© ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
    qc_text = st.text_area("Ø§Ù„ØµÙ‚ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§", st.session_state.get("last_article_md",""), height=300)
    col_q1, col_q2, col_q3 = st.columns(3)
    with col_q1:
        do_fluff = st.checkbox("ÙƒØ´Ù Ø§Ù„Ø­Ø´Ùˆ ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù„Ø¨ÙŠØ©", value=True)
    with col_q2:
        do_eeat = st.checkbox("Ù…Ø¤Ø´Ø±Ø§Øª E-E-A-T", value=True)
    with col_q3:
        do_llm_review = st.checkbox("ØªØ´Ø®ÙŠØµ Ù…ÙØ±Ø´Ø¯ (LLM)", value=True)

    if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹"):
        if not qc_text.strip():
            st.warning("Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            rep = quality_report(qc_text)
            st.session_state["qc_report"] = rep
            st.markdown("### Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø¯Ø±Ø¬Ø§Øª")
            colA, colB, colC = st.columns(3)
            with colA: st.metric("Human-style Score", rep["human_style_score"])
            with colB: st.metric("Sensory Ratio", rep["sensory_ratio"])
            with colC: st.metric("Fluff Density", rep["fluff_density"])
            st.markdown("#### ØªÙ†ÙˆÙ‘Ø¹ Ø§Ù„Ø¬Ù…Ù„"); st.json(rep["sentence_variety"])
            if do_eeat:
                st.markdown("#### E-E-A-T"); st.json({"presence": rep["eeat"], "score": rep["eeat_score"]})
                st.markdown("#### Information Gain"); st.json({"score": rep["info_gain_score"]})
            if do_fluff:
                st.markdown("#### Ø¹Ø¨Ø§Ø±Ø§Øª Ù‚Ø§Ù„Ø¨ÙŠØ© Ù…Ø±ØµÙˆØ¯Ø©")
                boiler = rep.get("boilerplate_flags") or []
                if boiler:
                    for f in boiler:
                        pattern = f.get("pattern", "?")
                        excerpt = f.get("excerpt", "")
                        st.write(f"- **Ù†Ù…Ø·:** `{pattern}` â€” Ù…Ù‚ØªØ·Ù: â€¦{excerpt}â€¦")
                else:
                    st.caption("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ø¨Ø§Ø±Ø§Øª Ù‚Ø§Ù„Ø¨ÙŠØ© Ø¸Ø§Ù‡Ø±Ø©.")

                repeats = rep.get("repeated_phrases") or []
                if repeats:
                    st.markdown("#### Ø¹Ø¨Ø§Ø±Ø§Øª Ù…ØªÙƒØ±Ø±Ø© Ø¨Ø´ÙƒÙ„ Ø²Ø§Ø¦Ø¯")
                    for g, c in repeats:
                        st.write(f"- `{g}` Ã— {c}")
            st.success("Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹.")
            st.session_state["qc_text"] = qc_text

    if do_llm_review and st.button("ğŸ§  ØªØ´Ø®ÙŠØµ Ù…ÙØ±Ø´Ø¯ (LLM)"):
        if not qc_text.strip():
            st.warning("Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ù‹Ø§.")
        elif not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
        else:
            client = get_client()
            out = llm_review(client, primary_model, fallback_model, qc_text)
            st.markdown("### ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØ±Ø§Ø¬Ø¹"); st.markdown(out)
            st.session_state["qc_review_md"] = out

    st.markdown("---")
    st.markdown("#### Ø¥ØµÙ„Ø§Ø­ Ø°ÙƒÙŠ Ù„Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù„Ù‘Ù…Ø©")
    flagged_block = st.text_area("Ø£Ù„ØµÙ‚ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ø³ÙŠÙ†Ù‡Ø§ (Ø³Ø·Ø± Ù„ÙƒÙ„ Ù…Ù‚Ø·Ø¹)", height=140, placeholder="Ø§Ù†Ø³Ø® Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø¶Ø¹ÙŠÙØ© ÙˆØ¶Ø¹Ù‡Ø§ Ù‡Ù†Ø§â€¦")
    if st.button("âœï¸ Ø£Ø¹ÙØ¯ Ø§Ù„ØµÙŠØ§ØºØ© Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø·"):
        if not flagged_block.strip():
            st.warning("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø£ÙˆÙ„Ù‹Ø§.")
        elif not qc_text.strip():
            st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ø£Ø³Ø§Ø³ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©.")
        elif not _has_api_key():
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ OPENAI_API_KEY.")
        else:
            client = get_client()
            new_text = llm_fix(client, primary_model, fallback_model, qc_text, flagged_block.splitlines())
            st.markdown("### Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ØµÙ„Ø§Ø­"); st.markdown(new_text)
            st.session_state["last_article_md"] = new_text
            st.success("ØªÙ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠ.")

# ------------------ Tab 4: Google Places (Ø§Ù„Ø¬Ø¯ÙŠØ¯) ------------------
with tab_places:
    st.subheader("Ø§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø·Ø§Ø¹Ù… Ø¹Ø¨Ø± Google Places")

    # Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ£ÙƒØ¯ Ø£Ù†Ùƒ ÙˆØ¶Ø¹Øª places_core.py ÙÙŠ utils/integrations/
    from utils.integrations.places_core import (
        CITY_PRESETS,
        places_search_text,
        make_items_from_places,
    )

    # Ù…ÙØªØ§Ø­ Google Ù…Ù† Ø§Ù„Ø£Ø³Ø±Ø§Ø±
    api_key = st.secrets.get("GOOGLE_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("GOOGLE_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ secrets.")
        st.stop()

    query = st.text_input("ğŸ” Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« (Ù…Ø«Ø§Ù„: Ø¨Ø±Ø¬Ø± Ø¨Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø¨Ø®Ø§Ø±ÙŠ Ø¬Ø¯Ø©...)")
    city_key = st.selectbox("ğŸ™ï¸ Ø§Ø®ØªØ± Ù…Ø¯ÙŠÙ†Ø©", list(CITY_PRESETS.keys()))
    max_results = st.number_input("ğŸ”¢ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†ØªØ§Ø¦Ø¬", min_value=1, max_value=20, value=10)
    min_reviews = st.number_input("â­ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª", min_value=0, value=50)

    if st.button("ğŸš€ Ø¬Ù„Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"):
        if not query:
            st.warning("Ø§ÙƒØªØ¨ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§ Ø£ÙˆÙ„Ù‹Ø§.")
        else:
            places = places_search_text(
                api_key,
                query,
                city_key,
                max_results=int(max_results),
            )

            region_code = CITY_PRESETS[city_key].get("regionCode", "SA")
            items_raw = make_items_from_places(
                api_key,
                places,
                min_reviews=int(min_reviews),
                region_code=region_code,
            )

            def convert_item(r: dict) -> dict:
                return {
                    "name": r.get("name", ""),
                    "address": r.get("address"),
                    "phone": r.get("phone"),
                    "thursday_hours": r.get("thursday_hours"),
                    "family_friendly": r.get("family_friendly") or "Ù†Ø¹Ù… (ØªÙ‚Ø¯ÙŠØ±ÙŠ)",
                    "price_per_person": r.get("price_range"),
                    "signature_dish": r.get("signature_dish") or "â€”",
                    "busy_times": r.get("crowd_note"),
                    "maps_url": r.get("maps_uri"),
                    "website": r.get("website"),
                }

            items = [convert_item(r) for r in items_raw]

            st.session_state["places_items"] = items
            st.session_state["places_index"] = { normalize_ar(it["name"]): it for it in items }

            st.success(f"ØªÙ… Ø¬Ù„Ø¨ {len(items)} Ù…Ø·Ø¹Ù…Ù‹Ø§.")
            if items:
                try:
                    import pandas as pd
                    df = pd.DataFrame([{
                        "Ø§Ù„Ø§Ø³Ù…": it["name"],
                        "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†": it["address"] or "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                        "Ø§Ù„Ø³Ø¹Ø± Ù„Ù„Ø´Ø®Øµ": it["price_per_person"] or "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                        "Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø²Ø­Ù…Ø©": it["busy_times"] or "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                    } for it in items])
                    st.dataframe(df, use_container_width=True)
                except Exception:
                    st.write("**Ø£ÙˆÙ„ 5 Ù†ØªØ§Ø¦Ø¬:**")
                    for it in items[:5]:
                        st.write("â€¢", it["name"], "â€”", (it["address"] or "ØºÙŠØ± Ù…ØªÙˆÙØ±"))

    if "places_items" in st.session_state and st.session_state["places_items"]:
        if st.button("â• Ø£Ø¶ÙÙ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¥Ù„Ù‰ Ø­Ù‚Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯"):
            restaurants_text = "\n".join([it["name"] for it in st.session_state["places_items"]])
            st.session_state["restaurants_text"] = restaurants_text
            st.success("ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¥Ù„Ù‰ ØªØ¨ÙˆÙŠØ¨ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ âœï¸ â€” Ø§ÙØªØ­Ù‡ Ø§Ù„Ø¢Ù†.")
